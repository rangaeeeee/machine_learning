\documentclass[14pt]{book}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[ampersand]{easylist}
\date{\today}
\begin{document}
\tableofcontents
\title{Machine Learning Algorithms}
\author{Rangarajan R}
\maketitle
\chapter{Regression Algorithms}
\section{Ordinary Least Squares Regression (OLSR)}
\section{Linear Regression}
\section{Logistic Regression}
\section{Stepwise Regression} 
\section{Multivariate Adaptive Regression Splines (MARS)}
\section{Locally Estimated Scatterplot Smoothing (LOESS)}
\chapter{Instance-based Algorithms}
\section{k-Nearest Neighbor (kNN)}
\section{Learning Vector Quantization (LVQ)}
\section{Self-Organizing Map (SOM)}
\section{Locally Weighted Learning (LWL)}
\chapter{Regularization Algorithms}
\section{Ridge Regression}
\section{Least Absolute Shrinkage and Selection Operator (LASSO)}
\section{Elastic Net}
\section{Least-Angle Regression (LARS)}
\chapter{Decision Tree Algorithms}
\section{Classification and Regression Tree (CART)}
\section{Iterative Dichotomiser 3 (ID3)}
\section{C4.5 and C5.0 (different versions of a powerful approach)}
\section{Chi-squared Automatic Interaction Detection (CHAID)}
\section{Decision Stump}
\section{M5}
\section{Conditional Decision Trees}
\chapter{Bayesian Algorithms}
\section{Naive Bayes}
\section{Gaussian Naive Bayes}
\section{Multinomial Naive Bayes}
\section{Averaged One-Dependence Estimators (AODE)}
\section{Bayesian Belief Network (BBN)}
\section{Bayesian Network (BN)}
\chapter{Clustering Algorithms}
\section{k-Means}
\section{k-Medians}
\section{Expectation Maximisation (EM)}
\section{Hierarchical Clustering}
\section{Association Rule Learning Algorithms}
\section{Apriori algorithm}
\section{Eclat algorithm}
\chapter{Artificial Neural Network Algorithms}
\section{Perceptron}
\section{Back-Propagation}
\section{Hopfield Network}
\section{Radial Basis Function Network (RBFN)}
\chapter{Deep Learning Algorithms}
\section{Deep Boltzmann Machine (DBM)}
\section{Deep Belief Networks (DBN)}
\section{Convolutional Neural Network (CNN)}
\section{Stacked Auto-Encoders}
\section{Recurrent Neural Network}
\chapter{Dimensionality Reduction Algorithms}
\section{Principal Component Analysis (PCA)}
\section{Principal Component Regression (PCR)}
\section{Partial Least Squares Regression (PLSR)}
\section{Sammon Mapping}
\section{Multidimensional Scaling (MDS)}
\section{Projection Pursuit}
\section{Linear Discriminant Analysis (LDA)}
\section{Mixture Discriminant Analysis (MDA)}
\section{Quadratic Discriminant Analysis (QDA)}
\section{Flexible Discriminant Analysis (FDA)}
\chapter{Ensemble Algorithms}
\section{Boosting}
\section{Bootstrapped Aggregation (Bagging)}
\section{AdaBoost}
\section{Stacked Generalization (blending)}
\section{Gradient Boosting Machines (GBM)}
\section{Gradient Boosted Regression Trees (GBRT)}
\section{Random Forest}
\section{K-Nearest Neighbors(KNN) for Machine Learning}
 The model representation for KNN is the entire training dataset. \\
 KNN has no model other than storing the entire dataset, so there is
  no learning required.
\section{Predictions using KNN}
\subsection{Euclidean distance}
\subsection{Hamming Distance}
\subsection{Manhattan Distance}
\subsection{Minkowski Distance}
\subsection{Tanimoto distance}
\subsection{Jaccard distance}
\subsection{Mahalanobis distance}
\subsection{Cosine distance}
\section{different disciplines have different names for KNN}
\subsection{Instance-Based Learning}
\subsection{Lazy Learning}
\subsection{Non-Parametric}
\section{Natrual Language Processing}
\end{document}